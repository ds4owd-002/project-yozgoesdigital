---
title: "Faecal sludge truck logistics analysis in Kampala, Uganda"
subtitle: "An exploration to understand the structure and trends of an informal faecal collection sector" 
author:
  name: "Jos van der Ent"
  orcid: 0009-0009-3940-3943
date: today
format: 
  html: 
    number-sections: true
    toc: true
    code-fold: true
    code-folding: true
crossref:
  chapters: true
bibliography: references.bib
execute: 
  echo: true #remove false before submitting as the requirements dictate to add the code to the report
  warning: true
license: "CC BY"
#csl: apa.csl # incase to use custom citation style
---

# Glossary {.unnumbered}

| Abbreviation | Comment       |
|:-------------|---------------|
| FS           | Faecal sludge |

# Introduction

The 'fslogisticskampala' data set is shared on [OpenWashdata](www.openwashdata.org) by Lars Sh√∂bitz. The goal of this dataset is to provide data sources on faecal sludge transporting logistics in Kampala, Uganda. The data is collected from 30th March 2015 until 25th June 2015 and contains the collection location and dislodge location. By reading the shared dataset and the written report the writer felt it would be interesting to see if there is more information and trends present in the dataset which has not be exposed in the narrative. This dataset is chosen for the Data Science for OpenWashData course. And this specific dataset as it contains location data, in form of GPS coordinates, and multiple datasets which can be joined for the analysis. Additionally it also contains time series which is a common aspect in dataset and so useful for getting hands-on experience

# METHODS: explore the raw data

## Import libraries
First the required libraries need to be imported to be able to read and process the datasets.

```{r libraries, message = FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(gt)
library(here)
library(gtsummary)
```

## Import trips data from 'fslogisticskampala'

The original set has 2 tables @lars_schobitz_fslogisticskampala_2024. One set contains information of trips of faecal sludge collection trucks and the second table with metadata on of the trucks. First we start with opening the trips.csv and save it as trips_raw.

Read trips.csv:

```{r read_trips}
#| message = FALSE 
trips_raw <- read_csv(here::here("data/raw/trips.csv"))
```

@tbl-trips-dictionary shows the dictionary of the trips dataset. 

```{r}
#| label: tbl-trips-dictionary
#| tbl-cap: "Dictionary for trips table"
#| message: FALSE 
read_csv(here::here("data/trips_dictionary.csv")) |>
  gt()
```
From the dictionary it can be seen the trips data has information about the each recorded trip. Where the 'numberplate'column can be used as the foreign key to link to the truck dataset as described below.

## Import trucks data from 'fslogisticskampala'
The second dataset to import is the trucks data as published in the website. Read trucks.csv:

```{r}
#| message = FALSE 
trucks_raw <- read_csv(here::here("data/raw/trucks.csv")) |>
  head(4)
  
```

@tbl-trucks-dictionary shows the dictionary of the trucks set. This is a more simpel table containing meta data on the truck. It only contains 2 variables. The first is the 'numberplate' which can function as key to link to the trips dataset and the second variable is the volume of the specific truck in m3.
```{r}
#| label: tbl-trucks-dictionary
#| tbl-cap: "Dictionary for trucks table"
#| message: FALSE 
read_csv(here::here("data/trucks_dictionary.csv")) |>
gt()
```

## Import dim_date
In order to aid analysis a additional table is generated and imported. This table has a daily geranuality and contains time intelligence information for the data collection period. 

## Checking data integrity
In order to understand the data we will first asses the raw data. First we will count all non NA values for each column (see @tbl-count-trips. In total 5663 records are in the dataset and it can be seen each column has no NA values. 

```{r}
#| label: tbl-count-trips
#| tbl-cap: "Count of "
# trips_tbl_sum <- trips_raw |>
  colSums(!is.na(trips_raw))
```

```{r}
trips_raw |> tbl_summary(numberplate)
```

#```{r}
#trips_tbl_sum <- waste_gt |>
#    filter(!is.na(generation_kg_capita))  |>
#    group_by(income_cat) |>
#    summarise(
#        count = n(),
#        mean = mean(generation_kg_capita),
#        sd = sd(generation_kg_capita),
#        median = median(generation_kg_capita),
#        min = min(generation_kg_capita),
#        max = max(generation_kg_capita)
#    )
#```

than generate the summary table:

#```{r}
#waste_tbl_income |>
#    gt() |>
#    tab_header(title = "Waste generation per capita (kg/year) by income group",
#               subtitle = "Data from 326 cities") |>
#    fmt_number(columns = count:max, decimals = 0) |>
#    cols_label(income_cat = "income category")
#```

```{r}
# trips_tbl_sum <- trips_raw |>
  colSums(!is.na(trucks_raw))
```

# METHODS: Generate Dataframe for analysis
<!---
Since the Mermaid plugin only seems to work on the highest chapter level this chapter is added as first heading.
-->
First the raw data is stored into the folder. This allows to make a versatile setup where cleaning steps can be incorporated into the used dataset when deemed necessary when analysis the results.
After saving the individual tables these will be linked according a 'star' schema based on the [@kimball_data_2013]. For the analysis the trips data is used as the fact table and the trucks and dim_date tables are linked as dimension tables over which the set can be grouped and filtered.

Generate a date dimension table and dimension time table in @fig-df_star_schema 


```{mermaid}
#| label: fig-df_star_schema
#| fig-cap: "Star schema of data frame"
erDiagram
    trips  }o--o| trucks  : numberplate
    trips }o--o| dim_date : date
    trucks {
        id numberplate	
        integer volume

    }
    trips {
        id fid
        string numberplate
        date date
        time time
        float lat
        float lon
        string plant
    }
    dim_date {
        date date
        integer day
        string day_suffix
        integer week_day
        string week_day_name
        string week_day_name-short
        string week_dayname_first-letter
        integer day_of_year
        integer week_of_month
        integer week_of_year
        integer month
        string month_name
        string month_name_Short
        string month_name_first-letter
        integer quarter
        string quarter_name
        integer year
        integer yyyymm
        string year_month
        binary is_weekend
        date first_date-of-month
        date last_date-of-month
        date first_date-of-week
        date last_date-of-week
        string year_week
    }
```

## Write data into folder with cleaned data

### trips

```{r}
trips_raw |>
  write_csv(here::here("data/processed/trips.csv"))
```
### trucks
```{r}
trucks_raw |>
  write_csv(here::here("data/processed/trucks.csv"))
```
Since the dim_date is generated for this specific reporting purpose it is directly put in the processed folder. This is because no cleaning steps are required and the data is directly usable.
# Results
Visuals:
map location with color on location

column graph with day of delivery

# Conclusions

# References

::: {#refs}
:::

# License

The original dataset on faecal sludge logistics was published under [CC-BY](https://github.com/openwashdata/fslogisticskampala/blob/main/LICENSE.md) The results of this report are under presented here under similar license:

# Ideas/actions

-   \[\] add license type for research
-   \[\] check weather data
-   \[\] day and night time (dim time)
-   \[\] plot on a map
-   \[\] check for NA values etc
-   \[\] [requirements](https://ds4owd-002.github.io/website/content/project/#tbl-required-items)
