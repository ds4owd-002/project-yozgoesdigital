---
title: "Faecal sludge truck logistics analysis in Kampala, Uganda"
subtitle: "An exploration to understand the structure and trends of an informal faecal collection sector" 
author:
  name: "Jos van der Ent"
  orcid: 0009-0009-3940-3943
date: today
format: 
  html: 
    number-sections: true
    toc: true
    code-fold: true
    code-folding: true
crossref:
  chapters: true
bibliography: references.bib
execute: 
  echo: true # remove false before submitting as the requirements dictate to add the code to the report
  warning: true
license: "CC BY"
#csl: apa.csl # incase to use custom citation style
---

# Glossary {.unnumbered}

| Abbreviation | Comment       |
|:-------------|---------------|
| FS           | Faecal sludge |

# Introduction

The 'fslogisticskampala' data set is shared on [OpenWashdata](www.openwashdata.org) by Lars Sh√∂bitz. The goal of this dataset is to provide data sources on faecal sludge transporting logistics in Kampala, Uganda. The data is collected from 30th March 2015 until 25th June 2015 and contains the collection location and dislodge location. By reading the shared dataset and the written report the writer felt it would be interesting to see if there is more information and trends present in the dataset which has not be exposed in the narrative. This dataset is chosen for the Data Science for OpenWashData course. And this specific dataset as it contains location data, in form of GPS coordinates, and multiple datasets which can be joined for the analysis. Additionally it also contains time series which is a common aspect in dataset and so useful for getting hands-on experience

# METHODS

## Import libraries

First the required libraries need to be imported to be able to read and process the datasets.

```{r libraries, message = FALSE}
library(tidyverse)
library(dplyr)
library(ggplot2)
library(gt)
library(here)
library(gtsummary)
library(lubridate)
library(leaflet)
```

## Import trips data from 'fslogisticskampala'

The original set has 2 tables @lars_schobitz_fslogisticskampala_2024. One set contains information of trips of faecal sludge collection trucks and the second table with metadata on of the trucks. First we start with opening the trips.csv and save it as trips_raw:

```{r read_trips}
#| message = FALSE 
trips_raw <- read_csv(here::here("data/raw/trips.csv"))
```

@tbl-trips-dictionary shows the dictionary of the trips dataset.

```{r}
#| label: tbl-trips-dictionary
#| tbl-cap: "Dictionary for trips table"
#| message: FALSE 
read_csv(here::here("data/processed/trips_dictionary.csv")) |>
    gt()
```

From the dictionary it can be seen the trips data has information about the each recorded trip. Where the 'numberplate' column can be used as the foreign key to link to the truck dataset as described below.

## Import trucks data from 'fslogisticskampala'

The second dataset to import is the trucks data as published in the website. Read trucks.csv and save as trucks_raw

```{r}
#| message = FALSE 
trucks_raw <- read_csv(here::here("data/raw/trucks.csv"))
```

@tbl-trucks-dictionary shows the dictionary of the trucks set. This is a more simple table containing meta data on the truck. It only contains 2 variables. The first is the 'numberplate' which can function as key to link to the trips dataset and the second variable is the volume of the specific truck in m3.

```{r}
#| label: tbl-trucks-dictionary
#| tbl-cap: "Dictionary for trucks table"
#| message: FALSE 
read_csv(here::here("data/processed/trucks_dictionary.csv")) |>
gt()
```

## Import dim_date

In order to aid analysis a additional table is generated and imported. This table has a daily granularity and contains time intelligence information for the data collection period.

## Checking data integrity

In order to understand the data we will first asses the raw data. First we will count all non NA values for each column (see @tbl-count-trips. In total 5663 records are in the dataset and it can be seen each column has no NA values.

```{r}
#| label: tbl-count-trips
#| tbl-cap: "Count of "
# trips_tbl_sum <- trips_raw |>
  colSums(!is.na(trips_raw))
```

```{r}
#| message: FALSE
#| label: fig-count_of_delivery_per_truck
#| fig-cap: "Count of recorded trips per truck"
truck_dist <- trips_raw |>
  count(numberplate, name = "n_entries") |>
  arrange(desc(n_entries))|>
  mutate("Percentage of entries" = round(n_entries / sum(n_entries) * 100,2))

truck_dist
```
Of the total 35 trucks the truck with the highest amount of entries is only accounting for 5.6% of the entries. So with the data set is spread out enough to represent the truck drivers in Kampala. The trips dataset can be used as a fact table when doing the analysis.

The second dataset called 'trucks' can be used as dimension table for the trips data to retrieve meta data from the trucks. In this case only the volume. In total 35 trucks are registered and each has a reported volume as an attribute:

```{r}
  colSums(!is.na(trucks_raw))
```

## Write data into folder with cleaned data

### trips

Write the cleaned trips data/processed data as csv after cleaning.

```{r}
trips <-  trips_raw |>
  mutate(numberplate = as.character(numberplate)) |>
  write_csv(here::here("data/processed/trips.csv"))
```

### trucks
The raw table is read and with the necessary steps cleaned and stored as trucks for further analysis.
```{r}
trucks <- trucks_raw |>
  mutate(numberplate = as.character(numberplate)) |>
  mutate(size_cat = case_when(
           volume < 8 ~"small",
           volume < 12 ~"medium",
           volume < 40 ~"large",
           volume < 999 ~"HUGE!",
           TRUE ~ "unkown"
           ))|>
  write_csv(here::here("data/processed/trucks.csv"))
```

### dim_date and dim_time
Since the dim_date and dim_time are generated for this specific reporting purpose it is directly put in the processed folder. This can be done as no additional cleaning steps are required. After reading the document the dataframes need to be generated so they can easily be references in the code.

```{r}
#| message: FALSE 
dim_date <- read_csv(here::here("data/processed/dim_date.csv"))
dim_time <- read_csv(here::here("data/processed/dim_time.csv"))
```

### Generate Dataframe for analysis

First each cleaned dataset is stored into the folder 'data/processed folder'. This allows to make a versatile setup where cleaning steps can be incorporated into the used dataset when deemed necessary when analysis the results. After saving the individual tables these will be linked according a 'star' schema based on @kimball_data_2013. For the analysis the trips data is used as the fact table and the trucks and dim_date tables are linked as dimension tables over which the set can be grouped and filtered. @fig-df_star_schema shows the schematic of the star schema.

```{mermaid}
%%| label: fig-df_star_schema
%%| fig-cap: "Star schema of data frame"
erDiagram
    trips }o--o| trucks  : numberplate
    trips }o--o| dim_date : date
    trips }o--o| dim_time   : time
    trucks {
        id numberplate	
        integer volume
        string size_cat
    }
    trips {
        id fid
        string numberplate
        date date
        time time
        float lat
        float lon
        string plant
    }
    dim_date {
        date date
        integer day
        string day_suffix
        integer week_day
        string week_day_name
        string week_day_name_short
        string week_dayname_first_letter
        integer day_of_year
        integer week_of_month
        integer week_of_year
        integer month
        string month_name
        string month_name_Short
        string month_name_first_letter
        integer quarter
        string quarter_name
        integer year
        integer yyyymm
        string year_month
        binary is_weekend
        date first_date_of_month
        date last_date_of_month
        date first_date_of_week
        date last_date_of_week
        string year_week
    }
    dim_time{
        integer hour12
        integer hour24
        integer minute_of_hour
        integer second_of_minute
        integer elapsed_minutes
        integer elapsed_seconds
        string am_pm
        time hhmmss
        binary working_hours
        integer part_of_day_sort
        string part_of_day
    }
    
```

In order generate the star schema the code below can be executed. This code will basically do a left join for of each dimension table to the trips fact table. This generates a big table with 45 variables and 5653 tuples (or rows).

```{r}
df <- trips |>
  left_join(trucks, by = join_by(numberplate )) |> 
  left_join(dim_date, by = join_by(date) ) |>
  left_join(dim_time, by = join_by(time == time_key))
```

# Results
This chapter will dive into the results of the dataset. The first visual (@fig-map_collection_locations) shows the collections locations and the 2 treatment sites.

The registered trucks have variable volumes. @fig-distribution-size shows the distribution volumes of registered trucks.

```{r}
#| label: fig-distribution-size
#| fig-cap: "Distribution of truck sizes"

size_cat_order <- c("small","medium","large","HUGE!")

df |>
  mutate(size_cat = factor(size_cat, levels = size_cat_order))|>
ggplot(aes(y = numberplate,
           x = volume,
           color = size_cat
           )) +
  geom_point(shape = 4, size = 1, stroke = 2)+
  labs(
    title = "Truck Volume Distribution",
    subtitle = "Plot of all vehicles sizes, with colors showing size category",
    x = "Registered Trucks",
    y = "Volume (m3)",
    color = "Size Category"
  )
```
Looking at the distribution a large variance can be observed, but also different clusters of sizes can be distinguished. By grouping on typical sizes it can be expected different types transport are linked. For examples of different sizes also refer to @linda_strande_faecal_2014. To summarise the results @fig-distribution-size-bp shows the results in a boxplot and @tbl-trucksize_summary shows statistics of the created groups.
```{r}
#| label: fig-distribution-size-bp
#| fig-cap: "Distribution of truck sizes visualized in a boxplot"
df |>
  ggplot(aes(y = volume,
             x= ''
             )) +
  geom_boxplot()+
  geom_point(aes(color = size_cat))+
  labs(
    title = "Truck Volume Distribution in a Boxplot",
    subtitle = "Boxplot of all vehicles, with colors showing size category",
    x = "Registered Trucks",
    y = "Volume (m3)",
    color = "Size Category"
  )
```


```{r}
#| label: tbl-trucksize_summary
#| tbl-cap: "Summary table of truck sizes, by size category"
trucks |>
  mutate(size_cat = factor(size_cat, levels = size_cat_order))|>
group_by(size_cat) |>
summarise(count = n(), 
          mean = round(mean(volume),1), 
          median = median(volume), 
          min = min(volume), 
          max = max(volume),
          sd = round(sd(volume),1)
          )|>
        gt()|>
  tab_header(
    title = "Summary table of truck sizes, by size category")|>
  tab_options(
    table.width = pct(80),
    table.align = "center",
    heading.align = "center",
    table.border.top.width = px(2),
    table.border.top.color = "black",
    table.border.bottom.width = px(2),
    table.border.bottom.color = "black"
  )
```

Apart from the collection location the trips dataset also states to which treatment plant the truck went for dislodging. One called Bugolobi and the other Lubigi. Where Lubigi is located at the Northern ring and Bugolobi at the southern part of the downtown. In order to see whether the distance from the one collection location and the treatment plant is correlated all collection points are mapped in fig-map_collection_locations and colours according the dislodging location. Also the locations of the treatment plants have been added as a reference.
```{r}
#| label: fig-map_collection_locations
#| fig-cap: "Map with collection locations and the existing treatment plants"
map = leaflet(data = df) |>
   addTiles() |>
   addCircleMarkers(~lon, ~lat 
                    ,popup = ~as.character(plant)
                    , color = ~ifelse(plant == "Bugolobi" , "red", "blue")
                    , radius = 0.5
                    ) |>
  addMarkers(lng = ~c(32.6071673,32.5458844)
             ,lat = ~c(0.3190139,0.3472747)
             ,popup = ~c("Bugolobi FS treatment plant","Lubigi FS treatment plant")
             )
map
```
Based on the collection location one can observe the collection location has some correlation to the dislodging location as collection at the south and east tend to more dislodged at Bugolobi and collections from the North and West tend to be dislodged at Lubigi. Altough some correlation can be observed the collection location is clearly not the only factor for deciding whether to dislodge  at a certain treatment plant. 
```{r}
#| label: fig-map_collection_locations_cat-size
#| fig-cap: "Map with collection locations with color of truck size category"
pal <- colorFactor(
  palette = c("blue", "orange", "green", "red", "purple"),
  domain  = df$size_cat
)

map = leaflet(data = df) |>
   addTiles() |>
   addCircleMarkers(~lon, ~lat 
                    , color = ~pal(size_cat)
                    , radius = 0.5
                    ) |>
  addMarkers(lng = ~c(32.6071673,32.5458844)
             ,lat = ~c(0.3190139,0.3472747)
             ,popup = ~c("Bugolobi FS treatment plant","Lubigi FS treatment plant")
             )
map
```

```{r}
#| label: fig-collection_time
#| fig-cap: "Collection times per hour of the day"
ggplot(df,aes(x=hour24,
              fill = plant))+
  geom_bar()+
  labs(
    title = "Count of collection per hour",
    subtitle = "Count of collections, with colors showing plant at which the truck dislodged",
    x = "Hour of the day",
    y = "# of entries",
    color = "Dislodging location"
  )
  
```
By counting timestamps and grouping them per hour of the day one can observe the time most collections are taking place (see @fig-collection_time). Clearly it can be see most activity take place during the day. It starts to get busy from 5:00 in the morning and getting to almost no activity after 18:00. It also looks like both plant are equally used for disloding over the whole day. So no correlation of collection time and disloding location can be observed.

# Conclusions
- The dataset contains a high variable set of faecal sludge collection trucks. Most of the smaller more versatile trucks. 4 registered trucks have volumes larger than 60m3 which are rarely seen in such settings. It could even be wondered if these volumes are correct or if a mistake in units is made.
- The collection location and the dislodge location seem to have a correlation, but based on the spread it is not the only factor on which location the trucks will dislodge
- The time of collection and the dislodging location do not seem to show any correlation and so does not seem to be a factor on the decusion where the plant truck is dislodging.

# References

::: {#refs}
:::

# License

The original dataset on faecal sludge logistics was published under [CC-BY](https://github.com/openwashdata/fslogisticskampala/blob/main/LICENSE.md) The results of this report are under presented here under similar license:

# Ideas/actions

-   \[\] day and night time (dim time)
-   \[\] [requirements](https://ds4owd-002.github.io/website/content/project/#tbl-required-items)
